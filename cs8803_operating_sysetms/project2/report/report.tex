%%%%%%%%%%%%%%%%%%%%%%% preamble %%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[11pt,letterpaper]{article}
\usepackage{opex3}
\usepackage{hyperref}
\usepackage{amssymb}
\hypersetup{colorlinks=true,
		      urlcolor=blue}
\usepackage{graphicx}  
\usepackage{underscore}
%%%%%%%%%%%%%%%%%%%%%%% begin %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\title{Project 2: Experimental variation \\ {\small CS8803 Graduate introduction to Operating Systems}}
\author{\href{mailto:rdk@gatech.edu}{Rohan D. Kekatpure}}
\vspace{0.2cm}

\section*{Introduction}
Project 2 aims to use our familiarity with Project 1 and deepen our understanding of OS concepts. While relatively light on coding, the project does require careful thinking about the components of the operating systems that we can address by varying request patterns, file sizes and number of threads. 

The possibilities for these parameter variations are endless. However, in the spirit of real-world conference submissions, we present our most significant results. It is to be noted that the actual the server configs (e.g, L1/L2/L3 cache sizes, HDD data rates and seek times, network specs etc) are opaque to us. Additionally, the experimental results are interpolated, not actual. Consequently we've elected to do``black box'' testing and focus on relatively coarse parameter variations. 

Additionally, we've chosen to present analysis at the end of each section as opposed to at the end. This makes it easier to visually link the analyses and conclusions with the graphical representation. 
%%
\section{Experiment 1: File size variation with fixed number of IO bound threads}
\subsection{Rationale: Study of throughput and response times of a server with a single IO bound thread}
In this experiment, we've attempted to keep the server response disk-IO-bound by making the server single-threaded (so it is blocked on IO), and by making {\sc fixed_size} requests. We then vary the file size between 8 KiB to 32 MiB and study variation of the server throughput and response time as a function of the file size.
%%
\subsection{Graphical representation}
Figure~\ref{fig:expt1} shows that the throughput as well as the response time increases with file size. Fig~\ref{fig:expt1} (C) shows that throughput increases with response for a fixed number of threads.
\begin{figure}[!htb]
\centering
\includegraphics[width=6in]{../figures/expt1.pdf}
\caption{Variation of (A) throughput and (B) response time with file size with 1 server thread. (C) depicts throughput vs response time with file size as implicit parameter. Note the logarithmic $x$-axis on all three subfigures.\label{fig:expt1}}
\end{figure}
%%
\subsection{Analysis}
With only 1 server thread, client requests have to queue up, resulting in an increased response time. Figure~\ref{fig:expt1}(B) suggests that the increase in response size is {\em faster} than the increase in file size. This could be so due to the fact that smaller files tend to be limited by the HDD seek time whereas  larger files become limited by HDD data transfer rates. 

The throughput increases with file size. This could be because of multiple reasons: 
\begin{enumerate}
\item Lesser number of disk seeks: Once the beginning block is located, the IO is limited by HDD data transfer rate
\item  Data locality and consequently increased effectiveness of prefetching.
\end{enumerate}
My (untested) hypothesis is that the throughput will saturate around the data transfer rates of HDDs. Assuming a commodity HDDs, the data transfer rate value would be expected to be between 200 and 500 MBytes/sec. 
%%
\section{Experiment 2: Varying number of threads for 1 MiB request size}
\subsection{Rationale: Study of server throughput and response time for a medium-sized file}
In this experiment, we keep the request file size fixed at 1 MiB, but change the number of threads. We also make caching less effective by  using {\sc fixed_size} requests and observe the effect on response time and throughput. 
%%
\subsection{Graphical representation}
\begin{figure}[!tbp]
\centering
\includegraphics[width=6in]{../figures/expt2.pdf}
\caption{Variation of (A) throughput and (B) response time with number of server threads for a fixed 1 MiB request size. (C) depicts throughput vs response time with number of server threads as the implicit parameter. The last two points in (A) and (B) overlap in (C).\label{fig:expt2}}
\end{figure}
Figure~\ref{fig:expt2} shows that the how the throughput first increases and then saturates with increasing number of server threads. The trend is mirrored in response time as well. Fig~\ref{fig:expt2} (C) depicts that throughput {\em decreases} with increasing response time for fixed request size (this trend is opposite to that in Fig~\ref{fig:expt1} (C))     
%%
\subsection{Analysis}
The threads are still IO bound, but the data size is medium, so depending on if some files are recycled, there may still be some caching going on. With increasing number of threads, however, the cache will be polluted (1 MiB request size is greater than L1 and L2 cache in Intel Core i7 processor). However, there is possibility of some caching, so we dont expect the performance to degrade with increasing number of threads. 
%%
\section{Experiment 3: Varying number of threads for 32 MiB request size}
\subsection{Rationale: Throughput and response time variation with threads with request size $\gg$ cache size}
This experiment is identical to Experiment 2 except that we keep the request size fixed at 32 MiB. We then vary the threads and study the effect on throughput and response time. 
%%
\subsection{Graphical representation}
Figure~\ref{fig:expt3} shows how the throughput first increases with number of threads until $n=10$ and then decreases. Again, the trend is mirrored in response times in a percentage sense (i.e., $\delta T/T \approx \delta R/R$) for the last two data points.  
\begin{figure}[!tbp]
\centering
\includegraphics[width=6in]{../figures/expt3.pdf}
\caption{Variation of (A) throughput and (B) response time with number of server threads for a fixed 32 MiB request size.\label{fig:expt3}}
\end{figure}
\subsection{Analysis}
A 32 MiB request size with {\sc fixed_size} and multiple threads significanly diminishes the possibility of caching in L1--L3. As a result, the performance actually degrades as a function of the number of threads beyond $n = 10$. 

\section{Experiment 4: Variation of number of threads for a fixed file of 32 MiB}
\subsection{Rationale: Demonstration of cache pollution by varying the request pattern}
This experiments attempts to demonstrate the effects of caching. The parameters are same as experiment 3, but this time with request pattern = {\sc fixed_file}. We'll replot experiment 3 data on top of experiment 4 for an easier comparison.
%%
\subsection{Graphical representation}
Figure~\ref{fig:expt4} depicts overlapped throughputs and response times for 32 MiB requests. The set of requests depicted with solid circles is for pattern = {\sc fixed_file} and the one with empty circles is for pattern = {\sc fixed_size} (replotted from Fig~\ref{fig:expt3}). {\sc fixed_file} requests always outperform {\sc fixed_size} ones. However, the difference is more pronounced (almost 2$\times$) for larger number of threads $>$ 10. The trend is mirrored, in a percentage sense, in the response times as well. 
\begin{figure}[!tbp]
\centering
\includegraphics[width=6in]{../figures/expt4.pdf}
\caption{Variation of (A) throughput and (B) response time for varying number of server threads for a fixed file {\em and} fixed size of 32 MiB. The dotted lines with empty symbols show the same data in Fig~\ref{fig:expt3} for comparison.\label{fig:expt4}}
\end{figure}
\subsection{Analysis}
With same file being transferred repeatedly, the {\sc fixed_file} requests can take advantage of caching. With increasing number of threads, the cache pollution tends to degrade the relative performance of {\sc fixed_size} requests. Equivalently, the effectiveness of cache hits become more pronounced for {\sc fixed_file} requests. Finally, for 30 threads, the {\sc fixed_file} requests have nearly twice the throughput and half the response time of {\sc fixed_size} requests.
\end{document}




