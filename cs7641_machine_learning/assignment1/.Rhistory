res_train <- predict.boosting(ada_fit, newdata=dtrain)
res_test <- predict.boosting(ada_fit, newdata=dtest)
errorevol(ada_fit, newdata=dtest) -> evol.test
errorevol(ada_fit, newdata=dtrain) -> evol.train
plot.errorevol(evol.test, evol.train, lty=c(1,1))
cat(sprintf("======================ADABOOST RESULTS======================\n"))
cat(sprintf("Training error = %f, test error = %f\n", res_train$error, res_test$error))
ada_complexity_param <- 0.01
ada_maxdepth <- 5
ada_fit <- boosting(fm, data=dtrain, boos=T, mfinal=200,
coeflearn="Breiman",
control=rpart.control(maxdepth=ada_maxdepth))
res_train <- predict.boosting(ada_fit, newdata=dtrain)
res_test <- predict.boosting(ada_fit, newdata=dtest)
errorevol(ada_fit, newdata=dtest) -> evol.test
errorevol(ada_fit, newdata=dtrain) -> evol.train
plot.errorevol(evol.test, evol.train, lty=c(1,1))
cat(sprintf("======================ADABOOST RESULTS======================\n"))
cat(sprintf("Training error = %f, test error = %f\n", res_train$error, res_test$error))
source('~/work/code/coursework/fall2015_machine_learning/assignment1/adaboost_creditset.R')
evol.test$error
?write.csv
source('~/work/code/coursework/fall2015_machine_learning/assignment1/adaboost_creditset.R')
errdf <- data.frame(itrn=c(1 : nrow(evol.test$error)),
etrain=evol.train$error,
etest=evol.test$error)
write.csv(errdf, "results/ionosphere/error_vs_boosting_iterations.csv",
row.names=F)
errdf <- data.frame(itrn=c(1 : nrow(evol.test$error)),
etrain=evol.train$error,
etest=evol.test$error)
rm(list = ls())
setwd("/Users/rkekatpure/work/code/coursework/fall2015_machine_learning/assignment1")
par(mar=c(1,1,1,1))
## Source common utilities
source("utils.R")
load_creditset()
## Load neural network library
library(rpart)
library(adabag)
## Normalize training and testing data
ntrain <- nrow(rtrain$x)
ntest <- nrow(rtest$x)
normFactor <- max(rtrain$x)
nxtrain <- rtrain$x/normFactor
meanFactor <- mean(nxtrain)
nxtrain <- nxtrain - meanFactor
nxtest <- rtest$x/normFactor - meanFactor
## Create data frames with normalized data
dtrain <- data.frame(y=as.factor(rtrain$y), X=rtrain$x)
dtest <- data.frame(y=as.factor(rtest$y), X=rtest$x)
## Create a formula
fm = as.formula(paste("y ~ ", paste0("X.", colnames(rtrain$x), collapse="+")))
## First fit an unboosted decision tree using rpart
rfit <- rpart(formula=fm, data=dtrain, method="class",
minsplit=0, minbucket=0,
maxdepth=30)
## Model accuracy on training data
res_train <- predict(rfit, dtrain[, -1], type="class")
etrain <- 100 * sum(rtrain$y != res_train)/ntrain
## Model accuracy of test set
res_test <- predict(rfit, dtest[, -1], type="class")
etest <- 100 * sum(rtest$y != res_test)/ntest
# Get number of nodes
rpart_num_nodes <- nrow(rfit$frame)
cat(sprintf("======================RPART RESULTS======================\n"))
cat(sprintf("nodes = %d, Training error = %f, test error = %f\n",
rpart_num_nodes, etrain, etest))
## Ada boost
ada_complexity_param <- 0.01
ada_maxdepth <- 5
ada_fit <- boosting(fm, data=dtrain, boos=T, mfinal=200,
coeflearn="Breiman",
control=rpart.control(maxdepth=ada_maxdepth))
res_train <- predict.boosting(ada_fit, newdata=dtrain)
res_test <- predict.boosting(ada_fit, newdata=dtest)
errorevol(ada_fit, newdata=dtest) -> evol.test
errorevol(ada_fit, newdata=dtrain) -> evol.train
plot.errorevol(evol.test, evol.train, lty=c(1,1))
cat(sprintf("======================ADABOOST RESULTS======================\n"))
cat(sprintf("Training error = %f, test error = %f\n", res_train$error, res_test$error))
errdf <- data.frame(itrn=c(1 : nrow(evol.test$error)),
etrain=evol.train$error,
etest=evol.test$error)
evol.test$error
length(evol.test$error)
errdf <- data.frame(itrn=c(1 : length(evol.test$error)),
etrain=evol.train$error,
etest=evol.test$error)
write.csv(errdf, "results/ionosphere/error_vs_boosting_iterations.csv",
row.names=F)
write.csv(errdf, "results/ionosphere/boosting/error_vs_boosting_iterations.csv",
row.names=F)
source('~/work/code/coursework/fall2015_machine_learning/assignment1/adaboost_ionosphere.R')
ada_fit$trees[1]
class(ada_fit$trees[1])
class(rfit)
plot(rfit)
varImp(rfit)
source('~/work/code/coursework/fall2015_machine_learning/assignment1/adaboost_ionosphere.R')
rm(list = ls())
library("e1071")
library("caret")
run_model <- function(xtrain, ytrain, xtest, ytest,
kernel="radial", degree=1, gamma=gamma) {
# Build a data frame from training data
dat <- data.frame(x=xtrain, y=ytrain)
svmfit <- svm(y~., data=dat, kernel=kernel, cost=10,
degree=degree, gamma=gamma)
predicted <- predict(svmfit, xtest)
training_error <- 100.0 * sum(predict(svmfit, xtrain) != ytrain)/length(ytrain)
test_error <- 100.0 * sum(predict(svmfit, xtest) != ytest)/length(ytest)
# @return
c(training_error, test_error)
}
setwd("/Users/rkekatpure/work/code/coursework/fall2015_machine_learning/assignment1")
source("utils.R")
source("utils.R")
rm(list = ls())
source("utils.R")
load_ionosphere()
ntest <- nrow(rtrain$x)
ntrain <- nrow(rtest$x)
cat(sprintf("Training set has %d rows and %d columns\n", nrow(xtrain), ncol(xtrain)))
cat(sprintf("Test set has %d rows and %d columns\n", nrow(xtest), ncol(xtest)))
cat(sprintf("Test set has %d rows and %d columns\n",
nrow(rtest$x), ncol(rtest$x)))
threshold <- 0.2
alldata <- rbind(rtrain$x, rtest$x)
nzv <- nearZeroVar(alldata, saveMetrics=TRUE)
dim(nzv[nzv$percentUnique > threshold,])
above_threshold <- nzv[nzv$percentUnique > threshold, ]
nzvcols <- as.numeric(rownames(above_threshold))
threshold <- 0.2
alldata <- rbind(rtrain$x, rtest$x)
nzv <- nearZeroVar(alldata, saveMetrics=TRUE)
dim(nzv[nzv$percentUnique > threshold,])
above_threshold <- nzv[nzv$percentUnique > threshold, ]
nzvcols <- as.numeric(rownames(above_threshold))
xtrain_nzv <- rtrain$x[, nzvcols]
xtest_nzv <- rtest$x[, nzvcols]
errs <- run_model(rtrain$x[1 : N, ], rtrain$y[1 : N],
rtest$x, rtest$y,
kernel="radial",
degree=3,
gamma=1e-3)
source('~/work/code/coursework/fall2015_machine_learning/assignment1/svm_ionosphere.R')
library("e1071")
library("caret")
run_model <- function(xtrain, ytrain, xtest, ytest,
kernel="radial", degree=1, gamma=gamma) {
# Build a data frame from training data
dat <- data.frame(x=xtrain, y=ytrain)
svmfit <- svm(y~., data=dat, kernel=kernel, cost=10,
degree=degree, gamma=gamma)
predicted <- predict(svmfit, xtest)
training_error <- 100.0 * sum(predict(svmfit, xtrain) != ytrain)/length(ytrain)
test_error <- 100.0 * sum(predict(svmfit, xtest) != ytest)/length(ytest)
# @return
c(training_error, test_error)
}
# Main
# Set working directory
setwd("/Users/rkekatpure/work/code/coursework/fall2015_machine_learning/assignment1")
# load_mnist.R contains logic for loading the mnist dataset and displaying an
# individual digit by reshaping a given row of the training data X. To use the
# functions in the file, we have to 'source' the file. Its similar to the
# 'import' in Python
source("utils.R")
load_ionosphere()
# Take random sample out of training and test data
ntest <- nrow(rtrain$x)
ntrain <- nrow(rtest$x)
cat(sprintf("Training set has %d rows and %d columns\n",
nrow(rtrain$x), ncol(rtrain$x)))
cat(sprintf("Test set has %d rows and %d columns\n",
nrow(rtest$x), ncol(rtest$x)))
# Remove zero variance columns
threshold <- 0.2
alldata <- rbind(rtrain$x, rtest$x)
nzv <- nearZeroVar(alldata, saveMetrics=TRUE)
dim(nzv[nzv$percentUnique > threshold,])
above_threshold <- nzv[nzv$percentUnique > threshold, ]
nzvcols <- as.numeric(rownames(above_threshold))
xtrain_nzv <- rtrain$x[, nzvcols]
xtest_nzv <- rtest$x[, nzvcols]
errs <- run_model(rtrain$x[1 : N, ], rtrain$y[1 : N],
rtest$x, rtest$y,
kernel="radial",
degree=3,
gamma=1e-3)
errs <- run_model(rtrain$x, rtrain$y,
rtest$x, rtest$y,
kernel="radial",
degree=3,
gamma=1e-3)
rtrain$x
errs <- run_model(rtrain$x, rtrain$y,
rtest$x, rtest$y,
kernel="radial",
degree=3,
gamma=1e-3)
?svm
as.matrix(rtrain$x)
errs <- run_model(as.matrix(rtrain$x), rtrain$y,
as.matrix(rtest$x), rtest$y,
kernel="radial",
degree=3,
gamma=1e-3)
dat <- dataframe(x=as.matrix(rtrain$x), y = rtrain$y)
dat <- data.frame(x=as.matrix(rtrain$x), y = rtrain$y)
dat
rtrain$x
?as.matrix
as.matrix(rtrain$x, rownames.force=NA)
X <- as.matrix(rtrain$x)
colnames(X) <- NULL
X
Xtrain <- as.matrix(rtrain$x)
colnames(Xtrain) <- NULL
Xtest <- as.matrix(rtest$x)
colnames(Xtest) <- NULL
errs <- run_model(as.matrix(rtrain$x), rtrain$y,
as.matrix(rtest$x), rtest$y,
kernel="radial",
degree=3,
gamma=1e-3)
rm(list = ls())
# Load libraries
library("e1071")
library("caret")
run_model <- function(xtrain, ytrain, xtest, ytest,
run_model <- function(xtrain, ytrain, xtest, ytest,
kernel="radial", degree=1, gamma=gamma) {
# Build a data frame from training data
dat <- data.frame(x=xtrain, y=ytrain)
svmfit <- svm(y~., data=dat, kernel=kernel, cost=10,
degree=degree, gamma=gamma)
predicted <- predict(svmfit, xtest)
training_error <- 100.0 * sum(predict(svmfit, xtrain) != ytrain)/length(ytrain)
test_error <- 100.0 * sum(predict(svmfit, xtest) != ytest)/length(ytest)
# @return
c(training_error, test_error)
}
rm(list = ls())
# Load libraries
library("e1071")
library("caret")
run_model <- function(xtrain, ytrain, xtest, ytest,
kernel="radial", degree=1, gamma=gamma) {
# Build a data frame from training data
dat <- data.frame(x=xtrain, y=ytrain)
svmfit <- svm(y~., data=dat, kernel=kernel, cost=10,
degree=degree, gamma=gamma)
predicted <- predict(svmfit, xtest)
training_error <- 100.0 * sum(predict(svmfit, xtrain) != ytrain)/length(ytrain)
test_error <- 100.0 * sum(predict(svmfit, xtest) != ytest)/length(ytest)
# @return
c(training_error, test_error)
}
setwd("/Users/rkekatpure/work/code/coursework/fall2015_machine_learning/assignment1")
source("utils.R")
load_ionosphere()
# Take random sample out of training and test data
ntest <- nrow(rtrain$x)
ntrain <- nrow(rtest$x)
cat(sprintf("Training set has %d rows and %d columns\n",
nrow(rtrain$x), ncol(rtrain$x)))
cat(sprintf("Test set has %d rows and %d columns\n",
nrow(rtest$x), ncol(rtest$x)))
Xtrain <- as.matrix(rtrain$x)
colnames(Xtrain) <- NULL
Xtest <- as.matrix(rtest$x)
colnames(Xtest) <- NULL
errs <- run_model(Xtrain, rtrain$y,
Xtest, rtest$y,
kernel="radial",
degree=3,
gamma=1e-3)
Xtrain
Xtrain[,2]
Xtrain <- as.matrix(rtrain$x[,-2])
colnames(Xtrain) <- NULL
Xtest <- as.matrix(rtest$x)
colnames(Xtest) <- NULL
errs <- run_model(Xtrain, rtrain$y,
Xtest, rtest$y,
kernel="radial",
degree=3,
gamma=1e-3)
Xtrain <- as.matrix(rtrain$x[,-2])
colnames(Xtrain) <- NULL
Xtest <- as.matrix(rtest$x[, -2])
colnames(Xtest) <- NULL
errs <- run_model(Xtrain, rtrain$y,
Xtest, rtest$y,
kernel="radial",
degree=3,
gamma=1e-3)
errs
source('~/work/code/coursework/fall2015_machine_learning/assignment1/svm_ionosphere.R')
source('~/work/code/coursework/fall2015_machine_learning/assignment1/svm_ionosphere.R')
source('~/work/code/coursework/fall2015_machine_learning/assignment1/svm_ionosphere.R')
source('~/work/code/coursework/fall2015_machine_learning/assignment1/svm_ionosphere.R')
source('~/work/code/coursework/fall2015_machine_learning/assignment1/svm_ionosphere.R')
source('~/work/code/coursework/fall2015_machine_learning/assignment1/svm_ionosphere.R')
plot(costs, etest)
plot(costs, etest, log='x')
plot(costs, etest, log='x')
## Variation with gamma
gammas <- 10^seq(-4, 5, by=0.1)
etrain <- numeric(length(gammas))
etest <- numeric(length(gammas))
for (j in c(1:length(gammas))){
gamma <- gammas[j]
errs <- run_model(Xtrain, rtrain$y,
Xtest, rtest$y,
kernel="radial",
degree=3,
gamma=gamma,
cost=71)
etrain[i] <- errs[1]
etest[i] <- errs[2]
cat(sprintf("gamma = %f, training error = %0.2f, test error = %0.2f\n", gamma, errs[1], errs[2]))
}
df = data.frame(gamma=gammas, etrain=etrain, etest=etest)
write.csv(df, file="results/ionosphere/svm/error_vs_gamma.csv", row.names=F)
## Variation with gamma
gammas <- 10^seq(-4, 5, by=0.1)
etrain <- numeric(length(gammas))
etest <- numeric(length(gammas))
for (j in c(1:length(gammas))){
gamma <- gammas[j]
errs <- run_model(Xtrain, rtrain$y,
Xtest, rtest$y,
kernel="radial",
degree=3,
gamma=gamma,
cost=71)
etrain[j] <- errs[1]
etest[j] <- errs[2]
cat(sprintf("gamma = %f, training error = %0.2f, test error = %0.2f\n", gamma, errs[1], errs[2]))
}
df = data.frame(gamma=gammas, etrain=etrain, etest=etest)
write.csv(df, file="results/ionosphere/svm/error_vs_gamma.csv", row.names=F)
gammas <- 10^seq(-6, 0, by=0.1)
etrain <- numeric(length(gammas))
etest <- numeric(length(gammas))
for (j in c(1:length(gammas))){
gamma <- gammas[j]
errs <- run_model(Xtrain, rtrain$y,
Xtest, rtest$y,
kernel="radial",
degree=3,
gamma=gamma,
cost=71)
etrain[j] <- errs[1]
etest[j] <- errs[2]
cat(sprintf("gamma = %f, training error = %0.2f, test error = %0.2f\n", gamma, errs[1], errs[2]))
}
df = data.frame(gamma=gammas, etrain=etrain, etest=etest)
write.csv(df, file="results/ionosphere/svm/error_vs_gamma.csv", row.names=F)
plot(gammas, etest, log='x')
## Variation with gamma
gammas <- 10^seq(-6, 6, by=0.01)
etrain <- numeric(length(gammas))
etest <- numeric(length(gammas))
for (j in c(1:length(gammas))){
gamma <- gammas[j]
errs <- run_model(Xtrain, rtrain$y,
Xtest, rtest$y,
kernel="radial",
degree=3,
gamma=gamma,
cost=71)
etrain[j] <- errs[1]
etest[j] <- errs[2]
cat(sprintf("gamma = %f, training error = %0.2f, test error = %0.2f\n", gamma, errs[1], errs[2]))
}
df = data.frame(gamma=gammas, etrain=etrain, etest=etest)
write.csv(df, file="results/ionosphere/svm/error_vs_gamma.csv", row.names=F)
plot(gammas, etest, log='x')
## Variation with gamma
gammas <- 10^seq(-6, 2, by=0.01)
etrain <- numeric(length(gammas))
etest <- numeric(length(gammas))
for (j in c(1:length(gammas))){
gamma <- gammas[j]
errs <- run_model(Xtrain, rtrain$y,
Xtest, rtest$y,
kernel="radial",
degree=3,
gamma=gamma,
cost=71)
etrain[j] <- errs[1]
etest[j] <- errs[2]
cat(sprintf("gamma = %f, training error = %0.2f, test error = %0.2f\n", gamma, errs[1], errs[2]))
}
df = data.frame(gamma=gammas, etrain=etrain, etest=etest)
write.csv(df, file="results/ionosphere/svm/error_vs_gamma.csv", row.names=F)
plot(gammas, etest, log='x')
rm(list = ls())
setwd("/Users/rkekatpure/work/code/coursework/fall2015_machine_learning/assignment1")
source("utils.R")
load_ionosphere()
library(neuralnet)
library(nnet)
rm(list = ls())
setwd("/Users/rkekatpure/work/code/coursework/fall2015_machine_learning/assignment1")
source("utils.R")
load_ionosphere()
# Load neural network library
library(neuralnet)
library(nnet)
# calculate the length of the Wts vector
num_wts = function(p, M, K) {
# returns the length of the Wts vector
#
# p = ncol(X) # number of predictor variables
# M = size    # number of hidden units
# K = 1       # number of classes
return ((p + 1) * M + K * (M + 1))
}
# Create training and test sets from MNIST data
ntest <- nrow(rtrain$x)
ntrain <- nrow(rtest$x)
cat(sprintf("Training set has %d rows and %d columns\n", nrow(xtrain), ncol(xtrain)))
cat(sprintf("Training set has %d rows and %d columns\n", nrow(nxtrain), ncol(nxtrain)))
normFactor <- max(rtrain$x)
nxtrain <- xtrain/normFactor
meanFactor <- mean(nxtrain)
normFactor <- max(rtrain$x)
nxtrain <- xtrain/normFactor
nxtrain <- rtrain$x/normFactor
meanFactor <- mean(nxtrain)
nxtrain <- nxtrain - meanFactor
nxtest <- rtest$x/normFactor - meanFactor
cat(sprintf("Training set has %d rows and %d columns\n", nrow(nxtrain), ncol(nxtrain)))
cat(sprintf("Test set has %d rows and %d columns\n", nrow(nxtest), ncol(nxtest)))
# number of predictor variables
p <- ncol(nxtrain)
# x, y input the number of output classes
K <- 10
best_M <- 20
best_maxit <- 250
best_decay <- 0.5
tstart <- proc.time()
nnmodel <- nnet(x=nxtrain, y = class.ind(ytrain),
softmax=TRUE,
size=best_M, decay=best_decay, maxit=best_maxit,
# bag=TRUE,
MaxNWts=num_wts(p, best_M, K),
# Wts=runif(num_wts(p, best_M, K), -0.5, 0.5),
trace=TRUE)
class.ind(rtrain$y)
nnmodel <- nnet(x=nxtrain, y = class.ind(rtrain$y),
softmax=TRUE,
size=best_M, decay=best_decay, maxit=best_maxit,
# bag=TRUE,
MaxNWts=num_wts(p, best_M, K),
# Wts=runif(num_wts(p, best_M, K), -0.5, 0.5),
trace=TRUE)
tend <- proc.time()
elapsed <- tend[3] - tstart[3]
ytrain.predicted <- as.numeric(predict(nnmodel, newdata=nxtrain, type="class"))
predict(nnmodel, newdata=nxtrain, type="class")
ytrain.predicted <- predict(nnmodel, newdata=nxtrain, type="class")
ytrain.predicted <- as.factor(predict(nnmodel, newdata=nxtrain, type="class"))
train.error <- 100 * sum(ytrain.predicted != rtrain$y)/ntrain
ypred <- as.factor(predict(nnmodel, newdata=nxtest, type="class"))
test.error <- 100 * sum(ypred != ytest)/ntest
test.error <- 100 * sum(ypred != rtest$y)/ntest
cat(sprintf("training error %f, test error = %f, run time = %f\n",
train.error, test.error, elapsed))
source('~/work/code/coursework/fall2015_machine_learning/assignment1/nn_ionosphere.R')
source('~/work/code/coursework/fall2015_machine_learning/assignment1/nn_ionosphere.R')
source('~/work/code/coursework/fall2015_machine_learning/assignment1/nn_ionosphere.R')
source('~/work/code/coursework/fall2015_machine_learning/assignment1/nn_ionosphere.R')
source('~/work/code/coursework/fall2015_machine_learning/assignment1/nn_ionosphere.R')
source('~/work/code/coursework/fall2015_machine_learning/assignment1/nn_ionosphere.R')
source('~/work/code/coursework/fall2015_machine_learning/assignment1/nn_ionosphere.R')
source('~/work/code/coursework/fall2015_machine_learning/assignment1/nn_ionosphere.R')
source('~/work/code/coursework/fall2015_machine_learning/assignment1/nn_ionosphere.R')
source('~/work/code/coursework/fall2015_machine_learning/assignment1/nn_ionosphere.R')
source('~/work/code/coursework/fall2015_machine_learning/assignment1/nn_ionosphere.R')
source('~/work/code/coursework/fall2015_machine_learning/assignment1/nn_ionosphere.R')
source('~/work/code/coursework/fall2015_machine_learning/assignment1/nn_ionosphere.R')
source('~/work/code/coursework/fall2015_machine_learning/assignment1/nn_ionosphere.R')
source('~/work/code/coursework/fall2015_machine_learning/assignment1/nn_ionosphere.R')
source('~/work/code/coursework/fall2015_machine_learning/assignment1/knn_ionosphere.R')
source('~/work/code/coursework/fall2015_machine_learning/assignment1/knn_ionosphere.R')
source('~/work/code/coursework/fall2015_machine_learning/assignment1/knn_ionosphere.R')
source('~/work/code/coursework/fall2015_machine_learning/assignment1/adaboost_ionosphere.R')
source('~/work/code/coursework/fall2015_machine_learning/assignment1/adaboost_ionosphere.R')
rfit <- rpart(formula=fm, data=dtrain, method="class",
minsplit=0, minbucket=0,
maxdepth=3)
## Model accuracy on training data
res_train <- predict(rfit, dtrain[, -1], type="class")
etrain <- 100 * sum(rtrain$y != res_train)/ntrain
## Model accuracy of test set
res_test <- predict(rfit, dtest[, -1], type="class")
etest <- 100 * sum(rtest$y != res_test)/ntest
# Get number of nodes
rpart_num_nodes <- nrow(rfit$frame)
cat(sprintf("======================RPART RESULTS======================\n"))
cat(sprintf("nodes = %d, Training error = %f, test error = %f\n",
rpart_num_nodes, etrain, etest))
source('~/work/code/coursework/fall2015_machine_learning/assignment1/dtree_ionosphere.R')
source('~/work/code/coursework/fall2015_machine_learning/assignment1/dtree_ionosphere.R')
source('~/work/code/coursework/fall2015_machine_learning/assignment1/dtree_ionosphere.R')
source('~/work/code/coursework/fall2015_machine_learning/assignment1/knn_mnist.R')
